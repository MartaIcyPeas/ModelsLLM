{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel , prepare_model_for_kbit_training , get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fr_aides.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           aide_href  \\\n",
      "0  https://les-aides.fr/aide/cUFf3w/ddfip/frr-exo...   \n",
      "1  https://les-aides.fr/aide/RjNf3w/region-nouvel...   \n",
      "2  https://les-aides.fr/aide/RjNv3w/region-nouvel...   \n",
      "3  https://les-aides.fr/aide/cjmP3w/region-nouvel...   \n",
      "4  https://les-aides.fr/aide/cRWf3w/region-nouvel...   \n",
      "\n",
      "                               tag  \\\n",
      "0  All√®gement des charges fiscales   \n",
      "1                       Subvention   \n",
      "2                       Subvention   \n",
      "3                       Subvention   \n",
      "4        Prise en charge des co√ªts   \n",
      "\n",
      "                                             aid  \\\n",
      "0    FRR : exon√©ration d'imp√¥t sur les b√©n√©fices   \n",
      "1               Aide √† l'h√¥tellerie ind√©pendante   \n",
      "2  Aide √† l'h√¥tellerie de plein air ind√©pendante   \n",
      "3       Soutien aux h√©bergements - G√Ætes d'√©tape   \n",
      "4                      Objectif transmission TPE   \n",
      "\n",
      "                                             projets  \n",
      "0  Les entreprises cr√©√©es ou reprises en zone Fra...  \n",
      "1  Accompagner les projets de d√©veloppement d'h√¥t...  \n",
      "2  Accompagner les projets de d√©veloppement de ca...  \n",
      "3  Soutenir la cr√©ation ou la modernisation de g√Æ...  \n",
      "4  Favoriser la transmission des entreprises en N...  \n",
      "Index(['aide_href', 'tag', 'aid', 'projets'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ad1d50821147fdae5da201b21f25b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martateodoratrales/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/martateodoratrales/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1509: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038511e038e2493c82b646aa28066b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b911a34bd3e41828f594039b5703ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8549115061759949, 'eval_runtime': 227.4494, 'eval_samples_per_second': 8.239, 'eval_steps_per_second': 0.519, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab951f8d420e4b928cc06ce0c33f4b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6406155824661255, 'eval_runtime': 2155.4064, 'eval_samples_per_second': 0.869, 'eval_steps_per_second': 0.055, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78719ca18be047f5aca00b13b661d661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5542246103286743, 'eval_runtime': 229.7639, 'eval_samples_per_second': 8.156, 'eval_steps_per_second': 0.514, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d0d1d485364d4d8030b73740e77bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4253044128417969, 'eval_runtime': 315.5357, 'eval_samples_per_second': 5.939, 'eval_steps_per_second': 0.374, 'epoch': 4.0}\n",
      "{'loss': 0.7957, 'grad_norm': 2.2997381687164307, 'learning_rate': 1.4350282485875708e-05, 'epoch': 4.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7f3dfc0f994736b3b7c94b2b55cdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3245341181755066, 'eval_runtime': 206.3267, 'eval_samples_per_second': 9.083, 'eval_steps_per_second': 0.572, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f9be6272404d5bbf179b1327e8b58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25927257537841797, 'eval_runtime': 221.3598, 'eval_samples_per_second': 8.466, 'eval_steps_per_second': 0.533, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c75db7d4294361aebed99880591118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2661759853363037, 'eval_runtime': 230.1316, 'eval_samples_per_second': 8.143, 'eval_steps_per_second': 0.513, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb72485b44374719908b54ae2d88cc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1619272083044052, 'eval_runtime': 218.0867, 'eval_samples_per_second': 8.593, 'eval_steps_per_second': 0.541, 'epoch': 8.0}\n",
      "{'loss': 0.3315, 'grad_norm': 18.470346450805664, 'learning_rate': 8.700564971751413e-06, 'epoch': 8.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b01aa6da37497789484cb770ac96b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13669157028198242, 'eval_runtime': 220.3377, 'eval_samples_per_second': 8.505, 'eval_steps_per_second': 0.536, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6746c792c44428d91130e304517d29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12305154651403427, 'eval_runtime': 228.5675, 'eval_samples_per_second': 8.199, 'eval_steps_per_second': 0.516, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae31023d8924e62b1ffcf51621d5510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09621628373861313, 'eval_runtime': 1179.8651, 'eval_samples_per_second': 1.588, 'eval_steps_per_second': 0.1, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd5cf1024545deb82262c58c6b7400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08449530601501465, 'eval_runtime': 231.057, 'eval_samples_per_second': 8.111, 'eval_steps_per_second': 0.511, 'epoch': 12.0}\n",
      "{'loss': 0.1386, 'grad_norm': 0.4945200979709625, 'learning_rate': 3.0508474576271192e-06, 'epoch': 12.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8381abf20c5f48ceb07a236624163718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07695002108812332, 'eval_runtime': 221.5933, 'eval_samples_per_second': 8.457, 'eval_steps_per_second': 0.533, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550918234e144ea3b1905c0ed6caac41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07259970903396606, 'eval_runtime': 228.3828, 'eval_samples_per_second': 8.206, 'eval_steps_per_second': 0.517, 'epoch': 14.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3eb5bc3d5a478590969eef221ad0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07161016762256622, 'eval_runtime': 222.9146, 'eval_samples_per_second': 8.407, 'eval_steps_per_second': 0.529, 'epoch': 15.0}\n",
      "{'train_runtime': 31584.788, 'train_samples_per_second': 0.89, 'train_steps_per_second': 0.056, 'train_loss': 0.37260923008460783, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1770, training_loss=0.37260923008460783, metrics={'train_runtime': 31584.788, 'train_samples_per_second': 0.89, 'train_steps_per_second': 0.056, 'total_flos': 3724455450193920.0, 'train_loss': 0.37260923008460783, 'epoch': 15.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('fr_aides.csv', delimiter=';')\n",
    "\n",
    "\n",
    "# Map each unique tag to a numerical label\n",
    "tag_to_label = {tag: idx for idx, tag in enumerate(df['tag'].unique())}\n",
    "df['labels'] = df['tag'].map(tag_to_label)\n",
    "\n",
    "\n",
    "with open('tag_to_label.json', 'w') as f:\n",
    "    json.dump(tag_to_label, f)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(tag_to_label))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['projets'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True,  # Explicitly disable CUDA\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5c4b7846a948e083bfad52b381df2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ed0422d2fd4325939e5a8c9fd772a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07161016762256622, 'eval_runtime': 236.7788, 'eval_samples_per_second': 7.915, 'eval_steps_per_second': 0.498, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(labels, pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./results/model')\n",
    "tokenizer.save_pretrained('./results/tokenizer')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results/tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "tag_to_label = {tag: idx for idx, tag in enumerate(df['tag'].unique())}\n",
    "df['labels'] = df['tag'].map(tag_to_label)\n",
    "\n",
    "with open('tag_to_label.json', 'w') as f:\n",
    "    json.dump(tag_to_label, f)\n",
    "\n",
    "\n",
    "with open('tag_to_label.json', 'r') as f:\n",
    "    tag_to_label = json.load(f)\n",
    "\n",
    "\n",
    "label_to_tag = {v: k for k, v in tag_to_label.items()}\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    text_label = label_to_tag[predictions.item()]\n",
    "    return text_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fr_aides.csv', delimiter=';')\n",
    "\n",
    "sample_df = df.sample(n=500, random_state=42)\n",
    "\n",
    "test_texts = sample_df['projets'].tolist()\n",
    "true_labels = sample_df['tag'].tolist()\n",
    "\n",
    "\n",
    "model_name = \"./results/model\"  # Path to the fine-tuned model\n",
    "tokenizer_name = \"./results/tokenizer\"  # Path to the fine-tuned tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "with open('tag_to_label.json', 'r') as f:\n",
    "    tag_to_label = json.load(f)\n",
    "\n",
    "# Reverse the mapping to get label_to_tag\n",
    "label_to_tag = {v: k for k, v in tag_to_label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.982, 'precision': 0.9749900161030596, 'recall': 0.982, 'f1': 0.9764624186245748}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martateodoratrales/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def test_model(texts, true_labels):\n",
    "    predicted_labels = [predict(text) for text in texts]\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "results = test_model(test_texts, true_labels)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
